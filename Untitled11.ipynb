{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOz86K2TC+0YzHkBQ/o+SkM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_MYZL1x8Qj5V"},"outputs":[],"source":["1.What is statistics, and why is it important?\n","-Python Statistics Fundamentals\n","1\n","2\n","3\n","Introduction to Python Statistics\n","\n","Statistics is a branch of mathematics dealing with data collection, analysis, interpretation, and presentation. Using Python for statistics is an efficient way to process statistical data and interpret it in a form that's easy to understand. Python, with its powerful libraries and tools, can handle various statistical operations, from basic measures of central tendency to complex statistical tests.\n","\n","Descriptive Statistics with Python\n","\n","Descriptive statistics involve summarizing and organizing data so that it can be understood and visualized easily. Python provides multiple libraries for carrying out descriptive statistics, including:\n","\n","Python's built-in statistics library: This library offers a range of simple statistical functions like mean, median, and mode.\n","\n","NumPy: It is a fundamental package for scientific computing with Python. It provides functions to perform statistical calculations on arrays of data.\n","\n","SciPy: Built on NumPy, it extends its capabilities by adding more sophisticated functions for optimization, regression, and probability distributions.\n","\n","pandas: Ideal for data manipulation and analysis, it offers data structures like Series and DataFrame, making it easier to handle statistical data.\n","\n","Matplotlib: A plotting library for creating static, interactive, and animated visualizations in Python.\n","\n","Calculating Basic Statistics\n","\n","Python's statistics module includes functions to calculate basic statistical measures. Here are some examples:\n","\n","Mean\n","\n","The mean, or average, is calculated by summing all numbers in a list and dividing by the count of numbers.\n","\n","import statistics\n","data = [1, 2, 3, 4, 5]\n","mean = statistics.mean(data)\n","Median\n","\n","The median is the middle number in a sorted list of numbers.\n","\n","median = statistics.median(data)\n","Mode\n","\n","The mode is the most common number in a list.\n","\n","mode = statistics.mode(data)\n","Variance and Standard Deviation\n","\n","Variance measures the spread of the numbers in a list. The standard deviation is the square root of the variance.\n","\n","variance = statistics.variance(data)\n","std_dev = statistics.stdev(data)\n","Working with NumPy\n","\n","NumPy extends the capabilities of Python's built-in statistics by adding functions that work efficiently with arrays.\n","\n","import numpy as np\n","np_data = np.array(data)\n","np_mean = np.mean(np_data)\n","np_median = np.median(np_data)\n","np_std_dev = np.std(np_data)\n","Data Visualization with Matplotlib\n","\n","Visualizing data is crucial for understanding the underlying patterns and making sense of complex datasets. Matplotlib provides a variety of plotting options, such as histograms, box plots, and scatter plots.\n","\n","Histogram\n","\n","A histogram represents the distribution of data by forming bins along the range of the data and then drawing bars to show the number of observations that fall in each bin.\n","\n","import matplotlib.pyplot as plt\n","plt.hist(np_data)\n","plt.show()\n","Box Plot\n","\n","A box plot, or box-and-whisker plot, shows the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum.\n","\n","plt.boxplot(np_data)\n","plt.show()\n","Conclusion\n","\n","Python provides a robust set of libraries to perform statistical analysis and visualize data. Whether you're dealing with small datasets or large-scale data, Python's statistical tools can help you make sense of your data and extract meaningful insights\n","1\n","2\n","3\n",".\n","\n","1\n",": Python's built-in statistics library documentation.\n","2\n",": Real Python's guide on Python statistics fundamentals.\n","3\n",": GeeksforGeeks article on statistics with Python."]},{"cell_type":"markdown","source":["2.What are the two main types of statistics?\n","You’ve got Descriptive Statistics and Inferential Statistics as the two pillars in Python, just like in traditional stats.\n","\n","Descriptive Statistics deals with summarizing your data. You’ll see metrics like mean, median, mode, standard deviation, and range. Libraries like numpy and pandas are your best friends here.\n","\n","Inferential Statistics helps you draw conclusions about a population based on sample data. This includes hypothesis testing, confidence intervals, correlation, regression, and so on. Tools like scipy.stats and statsmodels really shine in this space.\n","\n","\n"],"metadata":{"id":"nW4Hi15qS_Dh"}},{"cell_type":"markdown","source":["3.What are descriptive statistics?\n","-Descriptive statistics in Python are tools that help you summarize, organize, and understand your data without making predictions or generalizations beyond what you have.\n","\n","Here are some of the most common metrics and techniques, along with how they're used in Python:\n","\n","Mean: The average of all values. → data.mean() (with pandas) or np.mean(data)\n","\n","Median: The middle value in a sorted dataset. → data.median() or np.median(data)\n","\n","Mode: The most frequently occurring value. → data.mode()\n","\n","Standard Deviation and Variance: Measure the spread or dispersion of data. → data.std() and data.var()\n","Minimum and Maximum: → data.min() and data.max()\n","\n","Range: Difference between max and min. → data.max() - data.min()\n","\n","Summary statistics: → data.describe() gives you a neat summary of many key stats.\n","\n","These calculations are often powered by libraries like pandas, numpy, and scipy. They’re your first step in data analysis—great for spotting trends, errors, or patterns before you dive into deeper analytics."],"metadata":{"id":"snjPZ1KFTaxb"}},{"cell_type":"markdown","source":["4.What is inferential statistics?\n","-Inferential statistics in Python is all about taking a sample of data and using it to make generalizations or predictions about a larger population. While descriptive statistics summarize what’s directly in front of you, inferential statistics help you go beyond the data you have.\n","\n","Here are some key concepts and how they show up in Python:\n","\n","Hypothesis Testing\n","\n","Used to test assumptions (e.g., does this new drug work better than the old one?).\n","\n","Tools: scipy.stats.ttest_ind(), statsmodels.stats modules\n","\n","Confidence Intervals\n","Gives a range in which we expect a population parameter (like a mean) to fall.\n","\n","Available through bootstrapping or with functions in statsmodels\n","\n","Regression Analysis\n","\n","Models the relationship between variables (e.g., predicting house price based on size).\n","\n","Tools: statsmodels.OLS, sklearn.linear_model.LinearRegression\n","\n","ANOVA (Analysis of Variance)\n","\n","Tests if there are significant differences between group means.\n","\n","Use scipy.stats.f_oneway()"],"metadata":{"id":"Yw0B1__nTuEn"}},{"cell_type":"markdown","source":["5.What is sampling in statistics?\n","-Sampling in statistics refers to the process of selecting a subset (sample) from a larger population to analyze and draw conclusions about the entire population. In Python, sampling is often performed using libraries like NumPy, pandas, or random. Here's a concise explanation and examples:\n","\n","Key Concepts of Sampling\n","Random Sampling: Selecting data points randomly from the population.\n","Stratified Sampling: Dividing the population into subgroups (strata) and sampling from each subgroup.\n","Systematic Sampling: Selecting every nth data point from the population.\n","Importance: Sampling reduces computational effort while maintaining statistical validity.\n","Examples of Sampling in Python\n","1. Using NumPy for Random Sampling\n","Copy the code\n","import numpy as np\n","\n","# Population\n","population = np.arange(1, 101)  # Numbers from 1 to 100\n","\n","# Random sample of size 10\n","sample = np.random.choice(population, size=10, replace=False)\n","print(\"Random Sample:\", sample)\n","\n","2. Using pandas for Sampling from a DataFrame\n","Copy the code\n","import pandas as pd\n","\n","# Creating a DataFrame\n","data = pd.DataFrame({'ID': range(1, 101), 'Value': np.random.randint(1, 100, 100)})\n","\n","# Randomly sampling 10 rows\n","sample_df = data.sample(n=10, random_state=42)\n","print(sample_df)\n","\n","3. Using random for Simple Sampling\n","Copy the code\n","import random\n","\n","# Population\n","population = list(range(1, 101))\n","\n","# Random sample of size 10\n","sample = random.sample(population, 10)\n","print(\"Random Sample:\", sample)\n","\n","Applications\n","Estimating population parameters (e.g., mean, variance).\n","Reducing dataset size for faster computation.\n","Training machine learning models on smaller datasets.\n","\n","Sampling is a powerful tool in statistics and Python makes it easy to implement with its versatile libraries!"],"metadata":{"id":"X7lqGONeUGKv"}},{"cell_type":"markdown","source":["6. What are the different types of sampling methods?\n","-In Python, sampling methods are commonly used in data analysis, statistics, and machine learning to select subsets of data from larger datasets. Here are the main types of sampling methods and how they can be implemented in Python:\n","\n","1. Random Sampling\n","Definition: Each data point has an equal chance of being selected.\n","Implementation:\n","Copy the code\n","import numpy as np\n","import pandas as pd\n","\n","# Example DataFrame\n","data = pd.DataFrame({'A': range(1, 11)})\n","\n","# Random sampling of 5 rows\n","random_sample = data.sample(n=5, random_state=42)\n","print(random_sample)\n","\n","2. Stratified Sampling\n","Definition: Ensures that specific subgroups (strata) are represented proportionally in the sample.\n","Implementation:\n","Copy the code\n","from sklearn.model_selection import train_test_split\n","\n","# Example DataFrame\n","data = pd.DataFrame({'A': range(1, 11), 'Group': ['X', 'Y'] * 5})\n","\n","# Stratified sampling based on 'Group'\n","train, test = train_test_split(data, test_size=0.4, stratify=data['Group'], random_state=42)\n","print(test)\n","\n","3. Systematic Sampling\n","Definition: Selects every k-th element from the population.\n","Implementation:\n","Copy the code\n","import numpy as np\n","\n","# Example data\n","data = np.arange(1, 101)\n","\n","# Systematic sampling: Select every 10th element\n","k = 10\n","systematic_sample = data[::k]\n","print(systematic_sample)\n","\n","4. Cluster Sampling\n","Definition: Divides the population into clusters and randomly selects entire clusters.\n","Implementation:\n","Copy the code\n","import pandas as pd\n","import numpy as np\n","\n","# Example DataFrame\n","data = pd.DataFrame({'A': range(1, 21), 'Cluster': np.repeat([1, 2, 3, 4], 5)})\n","\n","# Randomly select clusters\n","selected_clusters = np.random.choice([1, 2, 3, 4], size=2, replace=False)\n","cluster_sample = data[data['Cluster'].isin(selected_clusters)]\n","print(cluster_sample)\n","\n","5. Convenience Sampling\n","Definition: Selects samples based on ease of access or availability.\n","Implementation:\n","Copy the code\n","# Example: Selecting the first 5 rows from a dataset\n","convenience_sample = data.head(5)\n","print(convenience_sample)\n","\n","\n","These methods can be adapted based on your dataset and the specific requirements of your analysis. Always ensure the sampling method aligns with the goals of your study to avoid bias."],"metadata":{"id":"1yVZi6t2UZdC"}},{"cell_type":"markdown","source":["7.What is the difference between random and non-random sampling?\n","-Great question! The key difference between random and non-random sampling in Python (and statistics in general) comes down to how samples are selected from a population:\n","\n","🔹 Random Sampling\n","Every item has a known and equal chance of being selected.\n","\n","This helps avoid bias and makes results more generalizable.\n","\n","Tools in Python: random.sample(), numpy.random.choice()\n","\n","Example:\n","import random\n","data = [1, 2, 3, 4, 5, 6, 7, 8]\n","sample = random.sample(data, 3)  # Randomly selects 3 unique items\n","🔸 Non-Random Sampling\n","Not every item has an equal chance of selection. Often chosen based on convenience or researcher judgment.\n","\n","Can introduce bias, but sometimes necessary when resources or access are limited.\n","\n","Examples: convenience sampling, snowball sampling, judgmental sampling\n","\n","You could simply do:\n","# Convenience sampling: taking the first 3 items\n","sample = data[:3]\n","In short: Random sampling is ideal for fairness and statistical validity. Non-random sampling is easier but can skew results."],"metadata":{"id":"uFiKt52mYSTv"}},{"cell_type":"markdown","source":["8. Define and give examples of qualitative and quantitative data?\n","-Certainly! Here's a clear explanation of qualitative and quantitative data, along with examples in Python:\n","\n","1. Qualitative Data\n","Definition: Qualitative data represents non-numerical information that describes qualities, characteristics, or categories. It is often descriptive and cannot be measured in numbers.\n","Examples: Colors, names, labels, or any categorical data.\n","Python Example:\n","Copy the code\n","# Example of qualitative data\n","colors = [\"Red\", \"Blue\", \"Green\", \"Yellow\"]  # Categories of colors\n","cities = [\"Mumbai\", \"Delhi\", \"Chennai\", \"Kolkata\"]  # Names of cities\n","is_student = [True, False, True, False]  # Boolean categories\n","\n","2. Quantitative Data\n","Definition: Quantitative data represents numerical information that can be measured or counted. It is often used for mathematical calculations and statistical analysis.\n","Examples: Age, height, weight, temperature, or any numerical data.\n","Python Example:\n","Copy the code\n","# Example of quantitative data\n","ages = [25, 30, 35, 40]  # Numerical data representing age\n","temperatures = [98.6, 99.1, 97.8, 100.2]  # Body temperatures in Fahrenheit\n","scores = [85, 90, 78, 92]  # Exam scores\n","\n","Key Difference in Python Context\n","Qualitative Data: Often stored as strings (str) or boolean values (bool).\n","Quantitative Data: Typically stored as integers (int) or floating-point numbers (float).\n","\n","By understanding these distinctions, you can better organize and analyze your data in Python! 😊"],"metadata":{"id":"BBe1T233Y10U"}},{"cell_type":"markdown","source":["9.What are the different types of data in statistics?\n","-In statistics—and when using Python to work with data—we usually classify data into two broad categories, each with subtypes. Here's a breakdown that fits both traditional statistical thinking and how you'd handle the data in Python:\n","\n","1. Qualitative (Categorical) Data\n","Data that describes qualities or characteristics, often labels or categories.\n","\n","Nominal: No natural order. Example: \"red\", \"blue\", \"green\" or \"apple\", \"banana\" Python: Stored as strings or category type in pandas.\n","\n","Ordinal: Has a meaningful order, but no fixed spacing. Example: \"low\", \"medium\", \"high\" Python: You can use pandas.Categorical() with ordered=True.\n","2. Quantitative (Numerical) Data\n","Data that represents measurable quantities and can be ordered.\n","\n","Discrete: Countable values. Example: number of students, cars, pets Python: Typically stored as int (integers).\n","\n","Continuous: Any value within a range, including fractions or decimals. Example: height, weight, temperature Python: Stored as float (floating-point numbers).\n","\n","Knowing your data type helps you choose the right analysis method and tools. For instance, you wouldn’t calculate a mean on a list of movie genres, but you would on a list of movie runtimes."],"metadata":{"id":"u_F5pWlZZDpM"}},{"cell_type":"markdown","source":["10.Explain nominal, ordinal, interval, and ratio levels of measurement?\n","-In statistics, understanding levels of measurement helps you choose the right tools for analysis. Python doesn't force you to label these levels explicitly, but your interpretation shapes how you analyze the data. Here’s a breakdown:\n","\n","🔹 1. Nominal Level\n","Categories without order—just labels or names.\n","\n","Example: \"red\", \"green\", \"blue\" or \"cat\", \"dog\"\n","\n","Python: Store as strings or use pd.Categorical(data) with no order.\n","import pandas as pd\n","colors = pd.Categorical(['red', 'blue', 'red', 'green'])\n","🔸 2. Ordinal Level\n","Categories with a clear, ranked order, but differences between them aren't measurable.\n","\n","Example: \"low\", \"medium\", \"high\"\n","\n","Python: Use pd.Categorical(data, ordered=True) for ranked data.\n","ratings = pd.Categorical(['low', 'medium', 'high'], ordered=True)\n","🔹 3. Interval Level\n","Numerical values with equal intervals, but no true zero. Differences are meaningful, but ratios aren't.\n","\n","Example: Temperature in Celsius or Fahrenheit (20°C is not “twice as hot” as 10°C)\n","\n","Python: Use numeric types (int, float) with clear context that there's no absolute zero.\n","\n","🔸 4. Ratio Level\n","All properties of interval data, plus a true zero point. You can calculate ratios.\n","\n","Example: Height, weight, age, income (zero means absence of the quantity)\n","Python: Also stored as int or float, but conceptually understood to have a meaningful zero.\n","\n","Think of it like this:\n","\n","Nominal: Naming things.\n","\n","Ordinal: Ranking things.\n","\n","Interval: Measuring things with equal steps.\n","\n","Ratio: Measuring things where “zero” means “none.”"],"metadata":{"id":"RREsVKNkZdb4"}},{"cell_type":"markdown","source":["11.What is the measure of central tendency?\n","-In Python, measures of central tendency are ways to describe the center or typical value of a dataset. The three most common are:\n","1. Mean – The average\n","Add up all the values and divide by the count.\n","import numpy as np\n","data = [10, 20, 30, 40]\n","mean = np.mean(data)\n","2. Median – The middle value\n","When the data is sorted, it's the number in the center (or the average of the two central values if there's an even number of items).\n","median = np.median(data)\n","3. Mode – The most frequent value\n","The value that appears most often in the dataset.\n","from scipy import stats\n","mode = stats.mode(data, keepdims=True).mode[0]\n","You can also use pandas for a more seamless experience, especially with DataFrames:\n","import pandas as pd\n","df = pd.DataFrame({'values': [10, 20, 20, 30, 40]})\n","mean = df['values'].mean()\n","median = df['values'].median()\n","mode = df['values'].mode()[0]\n"],"metadata":{"id":"8ibH-wDrZ4kj"}},{"cell_type":"markdown","source":["12.Define mean, median, and mode?\n","-In Python, mean, median, and mode are foundational tools for analyzing datasets—together, they tell you where your data “centers” and how values are distributed.\n","\n","📌 Mean (Average)\n","The sum of all values divided by the number of values.\n","import numpy as np\n","\n","data = [5, 10, 15, 20]\n","mean = np.mean(data)  # Output: 12.5\n","📌 Median (Middle Value)\n","The central value when your data is sorted.\n","median = np.median(data)  # Output: 12.5 if even-numbered, or middle value if odd\n","📌 Mode (Most Frequent Value)\n","The value(s) that appear most frequently.\n","from scipy import stats\n","\n","mode = stats.mode(data, keepdims=True).mode[0]  # Output: 5 (if it appears most often)\n","These are built into powerful Python libraries like numpy, pandas, and scipy, making statistical analysis super intuitive."],"metadata":{"id":"v-rxZkzPj_um"}},{"cell_type":"markdown","source":["13.What is the significance of the measure of central tendency?\n","-The measure of central tendency is like the heartbeat of your dataset—it tells you where the “center” of the data lies and helps you quickly grasp what’s typical or expected.\n","\n","Why it matters in Python (and stats in general):\n","Simplifies complex data: Instead of looking at every data point, you can look at one number (mean, median, or mode) to get the gist.\n","\n","Highlights data distribution: Knowing the mean vs. median can show whether your data is symmetric or skewed.\n","\n","Supports comparisons: Central values let you compare different groups—like test scores across classes or profits across months.\n","\n","Guides decision-making: From business forecasting to medical studies, central tendencies help make informed choices.\n","In Python, calculating these metrics is fast and easy with tools like numpy, pandas, and scipy. But beyond just math, they’re often your first compass when navigating raw, messy data."],"metadata":{"id":"Dx0FK_mbkXP1"}},{"cell_type":"markdown","source":["14. What is variance, and how is it calculated?\n","-Variance is a statistical measure that tells you how spread out the data values are around the mean. In other words, it quantifies the degree to which each number in a dataset differs from the average of the dataset.\n","\n","🧠 Why it matters:\n","Low variance: Data points are close to the mean (less spread).\n","\n","High variance: Data points are more spread out (more variability).\n","\n","🧮 Variance Formula:\n","Variance\n","=\n","∑\n","(\n","𝑥\n","𝑖\n","−\n","𝑥\n","ˉ\n",")\n","2\n","𝑛\n","(population)\n","or\n","∑\n","(\n","𝑥\n","𝑖\n","−\n","𝑥\n","ˉ\n",")\n","2\n","𝑛\n","−\n","1\n","(sample)\n","🐍 How to calculate it in Python:\n","Using NumPy:\n","import numpy as np\n","\n","data = [10, 12, 23, 23, 16, 23, 21, 16]\n","population_variance = np.var(data)\n","sample_variance = np.var(data, ddof=1)  # ddof=1 gives sample variance\n","Using Pandas:\n","import pandas as pd\n","\n","df = pd.DataFrame({'values': data})\n","sample_variance = df['values'].var()         # Sample variance\n","population_variance = df['values'].var(ddof=0)  # Population variance\n","The key is the ddof parameter:\n","\n","ddof=0 → population variance\n","\n","ddof=1 → sample variance"],"metadata":{"id":"poZ2KiyYklUr"}},{"cell_type":"markdown","source":["15.What is standard deviation, and why is it important?\n","-Standard deviation measures how spread out the numbers are in your dataset—it tells you how much individual data points differ from the mean.\n","\n","💡 Why It’s Important:\n","Low standard deviation → Data is closely clustered around the mean.\n","\n","High standard deviation → Data is more spread out; greater variability.\n","\n","It's essential for understanding consistency and reliability in data, such as scores, measurements, or predictions.\n","\n","Plays a key role in statistical analyses like confidence intervals, z-scores, and normal distributions.\n","🐍 How to Calculate It in Python:\n","Using NumPy:\n","import numpy as np\n","\n","data = [10, 12, 23, 23, 16, 23, 21, 16]\n","population_std = np.std(data)          # Population standard deviation\n","sample_std = np.std(data, ddof=1)      # Sample standard deviation (ddof=1)\n","Using Pandas:\n","import pandas as pd\n","\n","df = pd.DataFrame({'values': data})\n","std = df['values'].std()  # Sample standard deviation by default\n"],"metadata":{"id":"j4a1waktk9cu"}},{"cell_type":"markdown","source":["16. Define and explain the term range in statistics?\n","-In statistics, the range is a simple measure of spread that tells you the difference between the largest and smallest values in a dataset. It gives a quick sense of how spread out the values are, but it doesn’t show how the values are distributed between the extremes.\n","\n","🧮 Formula:\n","Range\n","=\n","Maximum Value\n","−\n","Minimum Value\n","🐍 In Python:\n","Using basic Python functions:\n","data = [5, 10, 15, 20, 25]\n","range_val = max(data) - min(data)\n","print(\"Range:\", range_val)  # Output: 20\n","Using Pandas:\n","import pandas as pd\n","df = pd.DataFrame({'values': data})\n","range_val = df['values'].max() - df['values'].min()\n","🔍 Why it matters:\n","It’s quick and easy to compute.\n","\n","Helps identify the total spread in your data.\n","\n","However, it’s sensitive to outliers, since it only considers the two endpoints."],"metadata":{"id":"85zNBuPilUQT"}},{"cell_type":"markdown","source":["17.What is the difference between variance and standard deviation?\n","-📊 Variance\n","Measures the average squared deviation from the mean.\n","\n","It gives you a sense of how much the values fluctuate, but because it’s squared, the units are not the same as the original data.\n","import numpy as np\n","\n","data = [5, 10, 15]\n","variance = np.var(data)           # Population variance\n","sample_variance = np.var(data, ddof=1)  # Sample variance\n","📉 Standard Deviation\n","This is the square root of the variance, bringing the measurement back to the original unit.\n","\n","Easier to interpret in the context of the data (e.g., \"on average, scores deviate by X points\").\n","std_dev = np.std(data)            # Population std\n","sample_std = np.std(data, ddof=1)  # Sample std\n","🎯 Key Difference:\n","Variance shows you how spread out your data is (in squared units).\n","\n","Standard deviation tells you how far, on average, each value is from the mean (in original units)."],"metadata":{"id":"PHWt3cFTlsC1"}},{"cell_type":"code","source":["18.What is skewness in a dataset?\n","-Skewness is a measure of the asymmetry in the distribution of your data. It tells you whether the data leans more to the left (negative skew) or to the right (positive skew) of the average.\n","\n","📈 Types of Skewness:\n","Positive skew (Right-skewed): Long tail on the right; most data is concentrated on the left.\n","\n","Negative skew (Left-skewed): Long tail on the left; most data is concentrated on the right.\n","\n","Zero skew: Perfectly symmetrical (like a normal distribution).\n","🐍 How to calculate it in Python:\n","Using scipy.stats:\n","from scipy.stats import skew\n","\n","data = [3, 4, 5, 6, 7, 8, 100]\n","skewness = skew(data)\n","print(\"Skewness:\", skewness)\n","A positive result → right-skewed\n","\n","A negative result → left-skewed\n","\n","Close to zero → approximately symmetric\n","\n","🔍 Why it matters:\n","Skewness helps identify outliers and data distortions.\n","\n","Many statistical tests (like linear regression) assume that data is normally distributed—so skewness can be a red flag.\n","\n","It affects how you interpret mean vs. median: in skewed data, the mean gets pulled toward the tail."],"metadata":{"id":"l7jvugqil5Y5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["19.What does it mean if a dataset is positively or negatively skewed?\n","-If a dataset is positively or negatively skewed, it's all about how the data is tilted when visualized, especially in relation to the mean and median.\n","\n","🔹 Positively Skewed (Right-Skewed)\n","The right tail is longer, and a few high values pull the mean to the right.\n","\n","Most data points are clustered on the lower end.\n","\n","Mean > Median > Mode\n","\n","📊 Think: Income data — most people earn modest amounts, but a few ultra-wealthy individuals pull the average up.\n","🔸 Negatively Skewed (Left-Skewed)\n","The left tail is longer, and low outliers pull the mean to the left.\n","\n","Most data points are clustered on the higher end.\n","\n","Mean < Median < Mode\n","\n","📊 Think: Age of retirement — most people retire around a certain range, but a few retire very early, pulling the average down.\n","🐍 In Python:\n","You can detect skewness using scipy.stats.skew:\n","from scipy.stats import skew\n","\n","data = [2, 3, 5, 7, 11, 15, 50]  # Example of positive skew\n","skewness = skew(data)\n","print(\"Skewness:\", skewness)\n","Positive value → right/positive skew\n","\n","Negative value → left/negative skew\n","\n","Close to 0 → approximately symmetric distribution"],"metadata":{"id":"a4wc2082mUmX"}},{"cell_type":"markdown","source":["20.Define and explain kurtosis?\n","-Kurtosis is a statistical measure that describes the “tailedness” of a distribution—essentially, how heavily the tails differ from those of a normal distribution. It tells us about the presence of outliers and how sharp or flat the peak of your data distribution is.\n","\n","🧠 Types of Kurtosis:\n","Mesokurtic (kurtosis ≈ 0)\n","\n","Similar to a normal distribution (e.g., bell curve).\n","\n","Moderate tails and peak.\n","\n","Leptokurtic (kurtosis > 0)\n","\n","Heavier tails and a sharper peak.\n","\n","Indicates more frequent outliers.\n","Platykurtic (kurtosis < 0)\n","\n","Flatter peak with lighter tails.\n","\n","Fewer outliers than a normal distribution.\n","\n","🐍 Calculating Kurtosis in Python\n","You can compute kurtosis using scipy.stats.kurtosis:\n","from scipy.stats import kurtosis\n","\n","data = [2, 3, 5, 7, 11, 13, 17, 19, 23]\n","kurt = kurtosis(data)\n","print(\"Kurtosis:\", kurt)\n","> By default, scipy.stats.kurtosis() uses Fisher’s definition, so a normal distribution has kurtosis = 0. If you want Pearson’s definition (where normal = 3), use fisher=False.\n","\n","python\n","kurt = kurtosis(data, fisher=False)  # Pearson’s definition\n","🔎 Why It’s Useful:\n","Helps detect outliers and risk in fields like finance, quality control, or social science.\n","Works alongside skewness to describe the shape of your data distribution."],"metadata":{"id":"yxHbpprZmqTx"}},{"cell_type":"markdown","source":["21.What is the purpose of covariance?\n","-Covariance is a statistical measure that helps you understand how two variables change together. In Python, it plays a key role in exploratory data analysis and is a foundational concept in things like portfolio theory, regression, and machine learning.\n","\n","🎯 Purpose of Covariance\n","Positive covariance: As one variable increases, the other tends to increase too.\n","\n","Negative covariance: As one increases, the other tends to decrease.\n","\n","Near zero: No consistent linear relationship between variables.\n","\n","It shows the direction of the relationship, but not the strength or scale—that’s where correlation comes in.\n","🐍 Calculating Covariance in Python\n","Using NumPy:\n","import numpy as np\n","\n","x = [2, 4, 6, 8]\n","y = [1, 3, 5, 7]\n","\n","cov_matrix = np.cov(x, y, ddof=1)\n","print(\"Covariance:\", cov_matrix[0, 1])\n","Using Pandas:\n","import pandas as pd\n","\n","df = pd.DataFrame({'x': x, 'y': y})\n","cov_value = df['x'].cov(df['y'])\n","print(\"Covariance:\", cov_value)\n","🔎 Why it matters\n","Covariance is used to:\n","\n","Detect linear relationships between variables\n","\n","Help build covariance matrices used in multivariate statistics\n","\n","Evaluate risk and return in financial modeling\n","\n","Serve as the backbone for principal component analysis (PCA)"],"metadata":{"id":"9uVj0RUinA-6"}},{"cell_type":"markdown","source":["22.What does correlation measure in statistics?\n","-In statistics—and in Python—correlation measures the strength and direction of the linear relationship between two variables.\n","\n","📏 What it tells you:\n","+1 → Perfect positive correlation (as one goes up, the other goes up).\n","\n","0 → No linear correlation.\n","\n","–1 → Perfect negative correlation (as one goes up, the other goes down).\n","\n","But keep in mind: correlation ≠ causation. Just because two variables move together doesn’t mean one causes the other!\n","🐍 How to Calculate Correlation in Python:\n","Using NumPy:\n","import numpy as np\n","\n","x = [1, 2, 3, 4, 5]\n","y = [10, 20, 30, 40, 50]\n","\n","corr_matrix = np.corrcoef(x, y)\n","print(\"Correlation:\", corr_matrix[0, 1])\n","Using Pandas:\n","import pandas as pd\n","\n","df = pd.DataFrame({'x': x, 'y': y})\n","correlation = df['x'].corr(df['y'])\n","print(\"Correlation:\", correlation)\n","🔍 Why it matters:\n","It's super handy in data exploration—spotting relationships between variables.\n","\n","It’s essential for building models, especially in machine learning, finance, and social science.\n","\n","It helps identify multicollinearity, which can distort regression models."],"metadata":{"id":"GF3GWnoqnVVO"}},{"cell_type":"markdown","source":["23.What is the difference between covariance and correlation?\n","-Covariance and correlation both measure how two variables move in relation to each other—but they differ in scale, interpretation, and use. Let's unpack it:\n","\n","📊 Covariance\n","Measures the direction of the linear relationship between two variables.\n","\n","A positive value means they increase together; a negative value means one increases as the other decreases.\n","\n","Scale-dependent: Its value is influenced by the units of the variables, making it hard to compare across different datasets.\n","Python example:\n","import numpy as np\n","\n","x = [1, 2, 3, 4]\n","y = [10, 20, 30, 40]\n","\n","cov_matrix = np.cov(x, y, ddof=1)\n","print(\"Covariance:\", cov_matrix[0, 1])\n","🔗 Correlation\n","Measures both the direction and strength of a linear relationship.\n","\n","Standardized: Always between –1 and +1.\n","\n","+1 → strong positive correlation\n","\n","–1 → strong negative correlation\n","\n","0 → no linear correlation\n","\n","Python example:\n","corr_matrix = np.corrcoef(x, y)\n","print(\"Correlation:\", corr_matrix[0, 1])\n","🧠 In short:\n","Covariance tells you which way variables move together.\n","\n","Correlation tells you how strongly they move together—and it's unitless."],"metadata":{"id":"GlfGHjSlnp8m"}},{"cell_type":"markdown","source":["24. What are some real-world applications of statistics?\n","-Statistics in Python powers a huge range of real-world applications—it's like the secret sauce behind smarter decisions, sharper predictions, and deeper insights. Here are some standout examples:\n","\n","🧠 1. Machine Learning & AI\n","Algorithms like linear regression, decision trees, and clustering rely heavily on statistical concepts.\n","\n","Libraries: scikit-learn, statsmodels, tensorflow\n","\n","📊 2. Data Science & Analytics\n","From analyzing customer behavior to predicting sales trends, statistics is the backbone.\n","\n","Tools: pandas, numpy, matplotlib, seaborn\n","\n","💰 3. Finance & Investment\n","Used for risk analysis, portfolio optimization, and forecasting stock prices.\n","\n","Techniques: time series analysis, correlation, regression\n","🏥 4. Healthcare & Epidemiology\n","Analyzing patient data, predicting disease outbreaks, and evaluating treatment effectiveness.\n","\n","Python helps crunch massive datasets from clinical trials and health records.\n","\n","🎬 5. Recommendation Systems\n","Think Netflix or Amazon—statistics helps personalize what you see based on your behavior.\n","\n","Techniques: collaborative filtering, Bayesian inference\n","🌐 6. Web & App Analytics\n","Track user engagement, A/B testing, and conversion rates using statistical tests.\n","\n","Python makes it easy to automate and visualize these insights.\n","\n","🚀 7. Scientific Research\n","Used in physics, biology, psychology, and more to validate hypotheses and analyze experiments.\n","\n","Python’s reproducibility and open-source tools make it a favorite in academia."],"metadata":{"id":"ya2cQQ_-n9wU"}},{"cell_type":"code","source":["PRACTICAL"],"metadata":{"id":"-kyFiJwsoVCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["1.How do you calculate the mean, median, and mode of a dataset?\n","-You can calculate the mean, median, and mode of a dataset in Python using built-in libraries like numpy, pandas, and scipy. Here’s a quick guide:\n","\n","🧮 Using NumPy and SciPy\n","import numpy as np\n","from scipy import stats\n","\n","data = [12, 18, 14, 10, 12, 20, 12]\n","\n","# Mean\n","mean = np.mean(data)\n","\n","# Median\n","median = np.median(data)\n","\n","# Mode\n","mode = stats.mode(data, keepdims=True).mode[0]\n","\n","print(\"Mean:\", mean)\n","print(\"Median:\", median)\n","print(\"Mode:\", mode)\n","📊 Using Pandas\n","import pandas as pd\n","\n","df = pd.DataFrame({'values': [12, 18, 14, 10, 12, 20, 12]})\n","\n","mean = df['values'].mean()\n","median = df['values'].median()\n","mode = df['values'].mode()[0]\n","\n","print(\"Mean:\", mean)\n","print(\"Median:\", median)\n","print(\"Mode:\", mode)\n","Each method gets the job done—numpy and scipy are great for quick stats, while pandas is awesome for working with structured data."],"metadata":{"id":"CFKZNjHCoWkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["2. Write a Python program to compute the variance and standard deviation of a dataset?\n","-Here’s a simple Python program that calculates both the variance and standard deviation of a dataset using both numpy and pandas. This way, you can pick whichever style fits your workflow better:\n","\n","✅ Using NumPy:\n","import numpy as np\n","\n","# Sample dataset\n","data = [12, 15, 20, 22, 26, 30, 35]\n","\n","# Compute variance\n","sample_variance = np.var(data, ddof=1)      # Sample variance\n","population_variance = np.var(data)          # Population variance\n","\n","# Compute standard deviation\n","sample_std_dev = np.std(data, ddof=1)\n","population_std_dev = np.std(data)\n","\n","# Display results\n","print(\"Sample Variance:\", sample_variance)\n","print(\"Population Variance:\", population_variance)\n","print(\"Sample Standard Deviation:\", sample_std_dev)\n","print(\"Population Standard Deviation:\", population_std_dev)\n","✅ Using Pandas:\n","import pandas as pd\n","\n","# Create a DataFrame\n","df = pd.DataFrame({'values': [12, 15, 20, 22, 26, 30, 35]})\n","\n","# Compute variance and standard deviation\n","variance = df['values'].var()              # Sample variance by default\n","std_dev = df['values'].std()\n","\n","# Display results\n","print(\"Sample Variance:\", variance)\n","print(\"Sample Standard Deviation:\", std_dev)\n"],"metadata":{"id":"EjT1iQBXtqeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["3.Create a dataset and classify it into nominal, ordinal, interval, and ratio types?\n","-Here's a small, custom dataset created in Python using pandas, along with the classification of each column into nominal, ordinal, interval, and ratio types.\n","\n","🐍 Create the Dataset:\n","import pandas as pd\n","\n","# Define the dataset\n","data = {\n","    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n","    'Education_Level': ['High School', 'Bachelor', 'Master', 'PhD'],\n","    'Temperature_C': [22, 25, 20, 23],\n","    'Salary': [40000, 55000, 65000, 80000]\n","}\n","\n","df = pd.DataFrame(data)\n","print(df)\n","📊 Classification of Data Types:\n","Column\t              Example Values\t          Level of Measurement\t  Explanation\n","\n","Name\t                Alice, Bob, Charlie     \tNominal\t               Names are labels with no inherent order\n","Education_Level\t      High School, Bachelor   \tOrdinal\t               Educational levels have a meaningful order\n","Temperature_C       \t22, 25, 20\t              Interval\t             Temperature has equal intervals but no true zero\n","Salary\t              40000, 55000, etc.\t      Ratio                  Salary has a true zero and allows ratio comparisons"],"metadata":{"id":"8pNqQ14LuNri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["4. Implement sampling techniques like random sampling and stratified sampling?\n","-Let’s walk through how to implement both random sampling and stratified sampling in Python using pandas and numpy.\n","\n","🎲 Random Sampling\n","This technique selects items from a dataset entirely at random.\n","import pandas as pd\n","\n","# Sample dataset\n","df = pd.DataFrame({\n","    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Ethan', 'Fiona'],\n","    'Age': [25, 30, 35, 40, 45, 50],\n","    'Gender': ['F', 'M', 'M', 'F', 'M', 'F']\n","})\n","\n","# Randomly sample 3 rows\n","random_sample = df.sample(n=3, random_state=42)\n","print(\"Random Sample:\\n\", random_sample)\n","🧪 Stratified Sampling\n","This ensures that each subgroup (stratum) is proportionally represented in the sample.\n","# Stratified sampling by 'Gender'\n","stratified_sample = df.groupby('Gender', group_keys=False).apply(lambda x: x.sample(frac=0.5, random_state=42))\n","print(\"\\nStratified Sample:\\n\", stratified_sample)\n","In this example, we sample 50% of each gender group. You can adjust frac or n depending on your needs."],"metadata":{"id":"tIe3wmK7vV4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["5. Write a Python function to calculate the range of a dataset?\n","-a simple Python function that calculates the range of a dataset, which is the difference between the maximum and minimum values:\n","def calculate_range(data):\n","    if not data:\n","        return None  # Handle empty list\n","    return max(data) - min(data)\n","\n","# Example usage:\n","dataset = [10, 20, 5, 40, 35]\n","range_val = calculate_range(dataset)\n","print(\"Range of dataset:\", range_val)\n","🔍 Output:\n","Range of dataset: 35\n","This function is straightforward but flexible—you can pass any list of numbers to it."],"metadata":{"id":"jduJavJcv17m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["6.Create a dataset and plot its histogram to visualize skewness?\n","-Here's a simple example where we create a skewed dataset and visualize its skewness using a histogram and KDE (Kernel Density Estimate) plot:\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import skew\n","\n","# Create a positively skewed dataset\n","np.random.seed(42)\n","data = np.random.exponential(scale=2.0, size=1000)\n","\n","# Convert to DataFrame\n","df = pd.DataFrame({'Values': data})\n","\n","# Calculate skewness\n","skewness_value = skew(df['Values'])\n","\n","# Plot histogram with KDE\n","plt.figure(figsize=(8, 5))\n","sns.histplot(df['Values'], bins=30, kde=True, color='skyblue', edgecolor='black')\n","plt.title(f'Histogram with KDE\\nSkewness = {skewness_value:.2f}')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","🧠 What you'll see:\n","A right-skewed (positively skewed) distribution with a long tail on the right.\n","\n","The skewness value printed in the title helps quantify the asymmetry."],"metadata":{"id":"dZOjXsFywJRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["7.Calculate skewness and kurtosis of a dataset using Python libraries?\n","-You got it! Here's how you can calculate both skewness and kurtosis of a dataset using Python’s scipy.stats and pandas libraries:\n","\n","✅ Using scipy.stats\n","from scipy.stats import skew, kurtosis\n","\n","# Sample dataset\n","data = [12, 15, 14, 10, 18, 20, 22, 25, 30, 35]\n","\n","# Calculate skewness and kurtosis\n","skewness = skew(data, bias=False)\n","kurt = kurtosis(data, bias=False)  # Fisher’s definition (normal = 0)\n","\n","print(\"Skewness:\", skewness)\n","print(\"Kurtosis:\", kurt)\n","✅ Using pandas\n","import pandas as pd\n","\n","df = pd.DataFrame({'values': data})\n","\n","# Calculate skewness and kurtosis\n","skewness = df['values'].skew()\n","kurt = df['values'].kurt()  # This uses Fisher’s definition too\n","\n","print(\"Skewness:\", skewness)\n","print(\"Kurtosis:\", kurt)\n","\n","\n","✅ Using pandas\n","import pandas as pd\n","\n","df = pd.DataFrame({'values': data})\n","\n","# Calculate skewness and kurtosis\n","skewness = df['values'].skew()\n","kurt = df['values'].kurt()  # This uses Fisher’s definition too\n","\n","print(\"Skewness:\", skewness)\n","print(\"Kurtosis:\", kurt)\n","🧠 Quick Interpretation:\n","Skewness > 0 → Right-skewed (tail on the right)\n","\n","Skewness < 0 → Left-skewed (tail on the left)\n","\n","Kurtosis > 0 → Leptokurtic (sharper peak, heavier tails)\n","\n","Kurtosis < 0 → Platykurtic (flatter peak, lighter tails)\n"],"metadata":{"id":"brQkhhdmwaH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["8.Generate a dataset and demonstrate positive and negative skewness?\n","-Here's a Python example that generates two datasets—one positively skewed and one negatively skewed—and visualizes them side by side using histograms and KDE plots:\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import skew\n","\n","# Set seed for reproducibility\n","np.random.seed(42)\n","\n","# Generate positively skewed data (e.g., exponential distribution)\n","positive_skew = np.random.exponential(scale=2.0, size=1000)\n","\n","# Generate negatively skewed data (e.g., reverse of exponential)\n","negative_skew = -np.random.exponential(scale=2.0, size=1000) + 10\n","\n","# Create DataFrame\n","df = pd.DataFrame({\n","    'Positive Skew': positive_skew,\n","    'Negative Skew': negative_skew\n","})\n","\n","# Calculate skewness\n","pos_skew_val = skew(df['Positive Skew'])\n","neg_skew_val = skew(df['Negative Skew'])\n","\n","# Plotting\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Positive skew\n","sns.histplot(df['Positive Skew'], bins=30, kde=True, ax=axes[0], color='skyblue')\n","axes[0].set_title(f'Positive Skew\\nSkewness = {pos_skew_val:.2f}')\n","axes[0].set_xlabel('Value')\n","axes[0].set_ylabel('Frequency')\n","\n","# Negative skew\n","sns.histplot(df['Negative Skew'], bins=30, kde=True, ax=axes[1], color='salmon')\n","axes[1].set_title(f'Negative Skew\\nSkewness = {neg_skew_val:.2f}')\n","axes[1].set_xlabel('Value')\n","axes[1].set_ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()\n","🧠 What you'll see:\n","The positively skewed plot has a long tail to the right.\n","\n","The negatively skewed plot has a long tail to the left.\n","\n","The skewness values quantify the asymmetry."],"metadata":{"id":"tuxUOYLtw6Dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["9.Write a Python script to calculate covariance between two datasets?\n","-Here's a simple Python script that calculates the covariance between two datasets using both numpy and pandas:\n","\n","✅ Using NumPy\n","import numpy as np\n","\n","# Sample datasets\n","x = [10, 20, 30, 40, 50]\n","y = [15, 25, 35, 45, 60]\n","\n","# Calculate covariance matrix\n","cov_matrix = np.cov(x, y, ddof=1)  # ddof=1 for sample covariance\n","\n","# Extract covariance value\n","covariance = cov_matrix[0, 1]\n","\n","print(\"Covariance (NumPy):\", covariance)\n","✅ Using Pandas\n","import pandas as pd\n","\n","# Create a DataFrame\n","df = pd.DataFrame({'X': x, 'Y': y})\n","\n","# Calculate covariance\n","covariance = df['X'].cov(df['Y'])\n","\n","print(\"Covariance (Pandas):\", covariance)\n","Both methods will give you the same result. NumPy returns a full covariance matrix, while Pandas gives you the direct value between two columns—super handy for dataframes."],"metadata":{"id":"rdnB8Nd1xSZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["10.Write a Python script to calculate the correlation coefficient between two datasets?\n","-Sure thing! Here's a Python script that calculates the Pearson correlation coefficient between two datasets using both numpy and pandas:\n","\n","✅ Using NumPy\n","import numpy as np\n","\n","# Sample datasets\n","x = [10, 20, 30, 40, 50]\n","y = [12, 24, 33, 47, 55]\n","\n","# Calculate correlation matrix\n","corr_matrix = np.corrcoef(x, y)\n","\n","# Extract correlation coefficient\n","correlation = corr_matrix[0, 1]\n","\n","print(\"Correlation Coefficient (NumPy):\", correlation)\n","✅ Using Pandas\n","import pandas as pd\n","\n","# Create a DataFrame\n","df = pd.DataFrame({'X': x, 'Y': y})\n","\n","# Calculate correlation\n","correlation = df['X'].corr(df['Y'])\n","\n","print(\"Correlation Coefficient (Pandas):\", correlation)\n","Both methods will give you the same result. NumPy returns a full correlation matrix, while Pandas gives you the direct value between two columns—super handy when working with DataFrames."],"metadata":{"id":"c6n2zkC8xrQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["11.Create a scatter plot to visualize the relationship between two variables?\n","-Absolutely! Here's a simple example using matplotlib and seaborn to create a scatter plot that visualizes the relationship between two variables—say, Hours Studied and Exam Score\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Sample dataset\n","data = {\n","    'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'Exam_Score': [50, 55, 58, 62, 65, 70, 75, 78, 85, 90]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Create scatter plot\n","plt.figure(figsize=(8, 5))\n","sns.scatterplot(data=df, x='Hours_Studied', y='Exam_Score', color='teal', s=100)\n","plt.title('Relationship Between Hours Studied and Exam Score')\n","plt.xlabel('Hours Studied')\n","plt.ylabel('Exam Score')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","🔍 What you'll see:\n","Each point represents a student.\n","\n","A clear upward trend suggests a positive correlation—more study hours, higher scores."],"metadata":{"id":"eVOpIHfkyp-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["12. Implement and compare simple random sampling and systematic sampling?\n","-Absolutely! Let's implement and compare Simple Random Sampling and Systematic Sampling using Python and pandas.\n","\n","🎲 Simple Random Sampling\n","Each item in the population has an equal chance of being selected.\n","import pandas as pd\n","\n","# Sample dataset\n","df = pd.DataFrame({\n","    'ID': range(1, 21),\n","    'Score': [88, 92, 79, 94, 86, 73, 91, 85, 89, 90, 76, 84, 95, 87, 82, 93, 78, 80, 81, 77]\n","})\n","\n","# Simple random sample of 5 rows\n","random_sample = df.sample(n=5, random_state=42)\n","print(\"Simple Random Sample:\\n\", random_sample)\n","📏 Systematic Sampling\n","Select every k-th item from the population after a random start.\n","import numpy as np\n","\n","# Define sample size\n","sample_size = 5\n","step = len(df) // sample_size\n","\n","# Choose a random start point\n","start = np.random.randint(0, step)\n","\n","# Select every k-th row\n","systematic_sample = df.iloc[start::step][:sample_size]\n","print(\"\\nSystematic Sample:\\n\", systematic_sample)\n","🔍 Comparison\n","Feature\t                 Simple Random Sampling\t            Systematic Sampling\n","\n","Selection Method\t       Randomly selected\t                Fixed interval after random start\n","Bias Risk\t               Low\t                              Can be biased if data has patterns\n","Reproducibility        \t Controlled via random_state\t      Depends on random start and step\n","Use Case\t               General-purpose sampling\t          Efficient for ordered data\n"],"metadata":{"id":"KV2JoqsKy_9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["13. Calculate the mean, median, and mode of grouped data?\n","-To calculate the mean, median, and mode of grouped data in Python, you’ll need to work with class intervals and frequencies. Here's how you can do it step by step:\n","\n","📊 Sample Grouped Data\n","# Class intervals and frequencies\n","classes = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50)]\n","frequencies = [12, 16, 6, 7, 9]\n","✅ 1. Mean of Grouped Data\n","# Calculate class marks (midpoints)\n","class_marks = [(low + high) / 2 for low, high in classes]\n","\n","# Compute mean\n","total_freq = sum(frequencies)\n","mean = sum(f * x for f, x in zip(frequencies, class_marks)) / total_freq\n","print(\"Mean:\", mean)\n","✅ 2. Median of Grouped Data\n","You’ll need cumulative frequencies and identify the median class.\n","import numpy as np\n","\n","# Cumulative frequencies\n","cum_freq = np.cumsum(frequencies)\n","n = total_freq\n","median_class_index = next(i for i, cf in enumerate(cum_freq) if cf >= n / 2)\n","\n","L = classes[median_class_index][0]  # Lower boundary of median class\n","f = frequencies[median_class_index]\n","CF = cum_freq[median_class_index - 1] if median_class_index > 0 else 0\n","h = classes[median_class_index][1] - classes[median_class_index][0]\n","\n","median = L + ((n / 2 - CF) / f) * h\n","print(\"Median:\", median)\n","✅ 3. Mode of Grouped Data\n","modal_class_index = frequencies.index(max(frequencies))\n","L = classes[modal_class_index][0]\n","f1 = frequencies[modal_class_index]\n","f0 = frequencies[modal_class_index - 1] if modal_class_index > 0 else 0\n","f2 = frequencies[modal_class_index + 1] if modal_class_index < len(frequencies) - 1 else 0\n","h = classes[modal_class_index][1] - classes[modal_class_index][0]\n","\n","mode = L + ((f1 - f0) / ((f1 - f0) + (f1 - f2))) * h\n","print(\"Mode:\", mode)\n","This approach follows the standard formulas for grouped data."],"metadata":{"id":"R91fIhlwzyJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["14.Simulate data using Python and calculate its central tendency and dispersion.?\n","-Here's a complete example where we simulate a dataset and calculate its central tendency (mean, median, mode) and dispersion (variance, standard deviation, range):\n","\n","🐍 Python Script\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","\n","# Simulate data: e.g., test scores of 100 students\n","np.random.seed(42)\n","data = np.random.normal(loc=70, scale=10, size=100)  # mean=70, std=10\n","\n","# Convert to DataFrame\n","df = pd.DataFrame({'Scores': data})\n","\n","# Central Tendency\n","mean = df['Scores'].mean()\n","median = df['Scores'].median()\n","mode = stats.mode(df['Scores'], keepdims=True).mode[0]\n","\n","# Dispersion\n","variance = df['Scores'].var()\n","std_dev = df['Scores'].std()\n","data_range = df['Scores'].max() - df['Scores'].min()\n","\n","# Display results\n","print(f\"Mean: {mean:.2f}\")\n","print(f\"Median: {median:.2f}\")\n","print(f\"Mode: {mode:.2f}\")\n","print(f\"Variance: {variance:.2f}\")\n","print(f\"Standard Deviation: {std_dev:.2f}\")\n","print(f\"Range: {data_range:.2f}\")\n","🧠 What This Shows:\n","Mean gives the average score.\n","\n","Median shows the middle value.\n","\n","Mode identifies the most frequent score.\n","\n","Variance and standard deviation measure how spread out the scores are.\n","\n","Range captures the difference between the highest and lowest scores."],"metadata":{"id":"384l3TLN0Q7X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["15.Use NumPy or pandas to summarize a dataset’s descriptive statistics?\n","-You can easily summarize a dataset’s descriptive statistics using either NumPy or pandas—both are powerful tools for exploring and understanding your data.\n","\n","✅ Using pandas (most convenient for DataFrames):\n","import pandas as pd\n","\n","# Sample dataset\n","data = {\n","    'Math': [85, 90, 78, 92, 88],\n","    'Science': [80, 85, 89, 94, 90]\n","}\n","df = pd.DataFrame(data)\n","\n","# Summary statistics\n","summary = df.describe()\n","print(summary)\n","This will give you:\n","\n","count, mean, std (standard deviation)\n","\n","min, 25%, 50% (median), 75%, and max\n","\n","You can also compute individual stats:\n","print(\"Mean:\\n\", df.mean())\n","print(\"Median:\\n\", df.median())\n","print(\"Mode:\\n\", df.mode().iloc[0])\n","✅ Using NumPy (great for arrays):\n","import numpy as np\n","\n","# Sample dataset\n","math_scores = np.array([85, 90, 78, 92, 88])\n","\n","# Descriptive stats\n","print(\"Mean:\", np.mean(math_scores))\n","print(\"Median:\", np.median(math_scores))\n","print(\"Standard Deviation:\", np.std(math_scores, ddof=1))\n","print(\"Min:\", np.min(math_scores))\n","print(\"Max:\", np.max(math_scores))\n","import numpy as np\n","\n","# Sample dataset\n","math_scores = np.array([85, 90, 78, 92, 88])\n","\n","# Descriptive stats\n","print(\"Mean:\", np.mean(math_scores))\n","print(\"Median:\", np.median(math_scores))\n","print(\"Standard Deviation:\", np.std(math_scores, ddof=1))\n","print(\"Min:\", np.min(math_scores))\n","print(\"Max:\", np.max(math_scores))\n","✅ Using NumPy (great for arrays):\n","import numpy as np\n","\n","🧠 Bonus: Use scipy.stats.describe() for a quick summary\n","from scipy.stats import describe\n","\n","result = describe(math_scores)\n","print(result)\n","This gives you count, min/max, mean, variance, skewness, and kurtosis in one go.\n"],"metadata":{"id":"-TFYa6Pr0jX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["16.Plot a boxplot to understand the spread and identify outliers?\n","-Absolutely! A boxplot is a great way to visualize the distribution, spread, and potential outliers in your data. Here's how you can do it using Python with matplotlib and seaborn:\n","🐍 Python Code Example\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Sample dataset\n","data = {\n","    'Scores': [55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 150]  # 150 is an outlier\n","}\n","df = pd.DataFrame(data)\n","\n","# Create boxplot\n","plt.figure(figsize=(6, 4))\n","sns.boxplot(x=df['Scores'], color='skyblue')\n","plt.title('Boxplot of Scores')\n","plt.xlabel('Score')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","🔍 What You’ll See:\n","The box shows the interquartile range (IQR: Q1 to Q3).\n","\n","The line inside the box is the median.\n","\n","The whiskers extend to the smallest and largest values within 1.5×IQR.\n","\n","Any dots outside the whiskers are considered outliers."],"metadata":{"id":"fmev47GU1RgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["17.Calculate the interquartile range (IQR) of a dataset?\n","-The Interquartile Range (IQR) measures the spread of the middle 50% of a dataset—it's the difference between the 75th percentile (Q3) and the 25th percentile (Q1). Here's how to calculate it in Python using both numpy and pandas:\n","\n","✅ Using NumPy\n","import numpy as np\n","\n","# Sample dataset\n","data = [12, 15, 14, 10, 18, 20, 22, 25, 30, 35]\n","\n","# Calculate Q1 and Q3\n","q1 = np.percentile(data, 25)\n","q3 = np.percentile(data, 75)\n","\n","# Compute IQR\n","iqr = q3 - q1\n","\n","print(\"Q1:\", q1)\n","print(\"Q3:\", q3)\n","print(\"Interquartile Range (IQR):\", iqr)\n","✅ Using Pandas\n","import pandas as pd\n","\n","df = pd.DataFrame({'values': data})\n","\n","# Compute IQR\n","q1 = df['values'].quantile(0.25)\n","q3 = df['values'].quantile(0.75)\n","iqr = q3 - q1\n","\n","print(\"Q1:\", q1)\n","print(\"Q3:\", q3)\n","print(\"Interquartile Range (IQR):\", iqr)\n","This is super useful for detecting outliers and understanding data spread."],"metadata":{"id":"ekKEjATl1meK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["18.Implement Z-score normalization and explain its significance?\n","-Z-score normalization—also known as standardization—is a technique that transforms your data so it has a mean of 0 and a standard deviation of 1. This is especially useful when your features are on different scales and you want them to contribute equally to a model.\n","\n","🧮 Why Z-score normalization matters:\n","Removes scale bias: Ensures features like height (in cm) and weight (in kg) don’t unfairly influence models.\n","\n","Essential for distance-based algorithms: Like k-NN, SVM, and PCA.\n","\n","Helps detect outliers: Z-scores far from 0 indicate unusual values.\n","🐍 Python Implementation\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'Height_cm': [150, 160, 165, 170, 180],\n","    'Weight_kg': [50, 60, 65, 70, 80]\n","})\n","\n","# Initialize the scaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","normalized_data = scaler.fit_transform(data)\n","\n","# Convert back to DataFrame for readability\n","normalized_df = pd.DataFrame(normalized_data, columns=data.columns)\n","print(normalized_df)\n","🧠 Interpretation:\n","Each value now represents how many standard deviations it is from the mean. A value of 0 means it's exactly average; +2 means it's two standard deviations above the mean."],"metadata":{"id":"E7KQBP3O15hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["19.Compare two datasets using their standard deviations?\n","-To compare two datasets using their standard deviations in Python, you're essentially assessing how spread out the values are in each dataset. Here's a simple script that does just that:\n","\n","🐍 Python Example\n","import numpy as np\n","import pandas as pd\n","\n","# Sample datasets\n","data1 = [12, 15, 14, 10, 18, 20, 22, 25, 30, 35]\n","data2 = [20, 21, 19, 22, 20, 21, 19, 22, 20, 21]\n","\n","# Convert to DataFrame (optional but handy)\n","df = pd.DataFrame({'Dataset1': data1, 'Dataset2': data2})\n","\n","# Calculate standard deviations\n","std1 = df['Dataset1'].std()\n","std2 = df['Dataset2'].std()\n","\n","print(f\"Standard Deviation of Dataset 1: {std1:.2f}\")\n","print(f\"Standard Deviation of Dataset 2: {std2:.2f}\")\n","\n","# Compare\n","if std1 > std2:\n","    print(\"Dataset 1 is more dispersed than Dataset 2.\")\n","elif std1 < std2:\n","    print(\"Dataset 2 is more dispersed than Dataset 1.\")\n","else:\n","    print(\"Both datasets have equal dispersion.\")\n","🧠 Why This Matters:\n","A higher standard deviation means the data is more spread out from the mean.\n","\n","This is useful in comparing variability—like test scores from two different classes or returns from two investment portfolios."],"metadata":{"id":"huH6skxr2MQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["20.Write a Python program to visualize covariance using a heatmap?\n","-Here's a Python program that calculates the covariance matrix of a dataset and visualizes it using a heatmap with seaborn and matplotlib:\n","\n","🐍 Python Script: Covariance Heatmap\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Sample dataset\n","data = {\n","    'Math': [85, 90, 78, 92, 88],\n","    'Science': [80, 85, 89, 94, 90],\n","    'English': [75, 70, 72, 78, 74]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Compute covariance matrix\n","cov_matrix = df.cov()\n","\n","# Plot heatmap\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(cov_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n","plt.title('Covariance Matrix Heatmap')\n","plt.tight_layout()\n","plt.show()\n","🔍 What You’ll See:\n","A heatmap where each cell shows the covariance between two variables.\n","\n","Diagonal values represent variances.\n","\n","Off-diagonal values show how two variables vary together."],"metadata":{"id":"ZX-s6O2n2aCE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["21. Use seaborn to create a correlation matrix for a dataset?\n","-You can easily create a correlation matrix heatmap using Seaborn in Python—it's a great way to visualize relationships between variables. Here's a complete example:\n","\n","🐍 Python Code: Correlation Matrix with Seaborn\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Sample dataset\n","data = {\n","    'Math': [85, 90, 78, 92, 88],\n","    'Science': [80, 85, 89, 94, 90],\n","    'English': [75, 70, 72, 78, 74]\n","}\n","df = pd.DataFrame(data)\n","\n","# Compute correlation matrix\n","corr_matrix = df.corr()\n","\n","# Plot heatmap\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n","plt.title('Correlation Matrix Heatmap')\n","plt.tight_layout()\n","plt.show()\n","🔍 What You’ll See:\n","Each cell shows the correlation coefficient between two variables.\n","\n","Values range from –1 (strong negative) to +1 (strong positive).\n","\n","The color intensity helps you spot strong relationships at a glance."],"metadata":{"id":"CNGQH4Lp2pHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["22. Generate a dataset and implement both variance and standard deviation computations?\n","-Here's a complete Python example that generates a dataset and computes both the variance and standard deviation using numpy and pandas:\n","\n","🐍 Python Script\n","import numpy as np\n","import pandas as pd\n","\n","# Generate a dataset: e.g., daily temperatures for 30 days\n","np.random.seed(42)\n","temperatures = np.random.normal(loc=30, scale=5, size=30)  # mean=30°C, std=5°C\n","\n","# Convert to DataFrame\n","df = pd.DataFrame({'Temperature_C': temperatures})\n","\n","# Using pandas\n","variance_pd = df['Temperature_C'].var()       # Sample variance\n","std_dev_pd = df['Temperature_C'].std()        # Sample standard deviation\n","\n","# Using numpy\n","variance_np = np.var(temperatures, ddof=1)    # ddof=1 for sample variance\n","std_dev_np = np.std(temperatures, ddof=1)\n","\n","# Display results\n","print(\"Pandas - Variance:\", round(variance_pd, 2))\n","print(\"Pandas - Standard Deviation:\", round(std_dev_pd, 2))\n","print(\"NumPy  - Variance:\", round(variance_np, 2))\n","print(\"NumPy  - Standard Deviation:\", round(std_dev_np, 2))\n","🧠 Why this matters:\n","Variance tells you how spread out the data is.\n","\n","Standard deviation gives you that spread in the same units as the data.\n","\n","Using both pandas and numpy shows how flexible Python is for statistical analysis."],"metadata":{"id":"gzcxer5K2-yp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["23.Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn?\n","-You can absolutely visualize skewness and kurtosis using Python libraries like matplotlib, seaborn, and scipy. Here's a complete example that simulates a dataset, calculates both metrics, and visualizes the distribution:\n","\n","🐍 Python Script\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import skew, kurtosis\n","\n","# Simulate a dataset\n","np.random.seed(42)\n","data = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution\n","\n","# Convert to DataFrame\n","df = pd.DataFrame({'Values': data})\n","\n","# Calculate skewness and kurtosis\n","skew_val = skew(df['Values'])\n","kurt_val = kurtosis(df['Values'])  # Excess kurtosis (normal = 0)\n","\n","# Plot histogram with KDE\n","plt.figure(figsize=(8, 5))\n","sns.histplot(df['Values'], bins=30, kde=True, color='skyblue', edgecolor='black')\n","plt.title(f'Distribution of Values\\nSkewness = {skew_val:.2f}, Kurtosis = {kurt_val:.2f}')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","🔍 What You’ll See:\n","Skewness tells you if the distribution leans left (negative) or right (positive).\n","\n","Kurtosis shows how peaked or flat the distribution is compared to a normal curve.\n","\n","The KDE curve helps you visually assess both."],"metadata":{"id":"y5fCWYVf3Nl4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["24. Implement the Pearson and Spearman correlation coefficients for a dataset.?\n","-Here's how to implement both Pearson and Spearman correlation coefficients in Python using pandas and scipy.stats. These are two of the most commonly used methods to measure the strength and direction of association between two variables.\n","\n","🧪 Sample Dataset\n","import pandas as pd\n","\n","# Create a sample dataset\n","data = {\n","    'X': [10, 20, 30, 40, 50],\n","    'Y': [12, 24, 33, 47, 55]\n","}\n","df = pd.DataFrame(data)\n","✅ Pearson Correlation (Linear Relationship)\n","python\n","from scipy.stats import pearsonr\n","\n","# Calculate Pearson correlation and p-value\n","pearson_corr, pearson_p = pearsonr(df['X'], df['Y'])\n","\n","print(f\"Pearson Correlation: {pearson_corr:.2f}\")\n","print(f\"P-value: {pearson_p:.4f}\")\n","✅ Spearman Correlation (Monotonic Relationship)\n","from scipy.stats import spearmanr\n","\n","# Calculate Spearman correlation and p-value\n","spearman_corr, spearman_p = spearmanr(df['X'], df['Y'])\n","\n","print(f\"Spearman Correlation: {spearman_corr:.2f}\")\n","print(f\"P-value: {spearman_p:.4f}\")\n","🧠 When to Use What:\n","Pearson: Use when the relationship is linear and variables are normally distributed.\n","\n","Spearman: Use when the relationship is monotonic (not necessarily linear) or when data is ordinal or not normally distributed."],"metadata":{"id":"FWQfry6i3ct5"},"execution_count":null,"outputs":[]}]}